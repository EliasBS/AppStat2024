{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting is an Art!\n",
    "\n",
    "Python macro for testing which fitting procedure is likely to give the \"best\" results.\n",
    "\n",
    "In this case, we consider a **double exponential distribution**, where there are (naturally) high correlations, which have to be addressed/managed through the design of the fitting function.\n",
    "\n",
    "### Authors:\n",
    "- Troels Petersen ([email](mailto:petersen@nbi.dk))\n",
    "\n",
    "### Last update:\n",
    "- 28th of December 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit, cost\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random\n",
    "r.seed(42)\n",
    "SavePlots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE: Double exponential distribution\n",
    "\n",
    "Here we are considering the fitting of exponential data, and how the writing of the fitting function is important.\n",
    "\n",
    "* The \"bad\" fitting function:\n",
    "    $f_{bad}(t)  = N_{1}\\cdot\\exp(-t/r_{1}) + N_{2}\\cdot\\exp(-t/r_{2})$ for $t$ in $[0,\\infty]$\n",
    "\n",
    "* The \"good\" fitting function:\n",
    "    $f_{good}(t) = N \\cdot\\left(\\frac{f}{r_{1}}\\cdot \\exp\\left[-t/r_{1}\\right] + \\frac{(1-f)}{r_{2}}\\cdot\\exp\\left[-t/r_{2}\\right]\\right)$ for $t$ in $[0,\\infty]$\n",
    "\n",
    "NOTE: The parameters $r_1$ and $r_2$ need to be positive, and $f$ in [0,1], in order for this to be a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npoints = 2000\n",
    "frac =  0.5               # Fraction that \"belongs to\" first exponential\n",
    "tau1 = 10.0\n",
    "tau2 =  2.0               # Note: The size of the lifetime ratio decides their correlation!\n",
    "Nbins = 200\n",
    "xmin = 0.0\n",
    "xmax = 20.0\n",
    "binwidth = (xmax - xmin) / Nbins\n",
    "print(f\"  The binwidth is {binwidth:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([r.exponential(scale=tau1, size=int(Npoints*frac)),\n",
    "                       r.exponential(scale=tau2, size=int(Npoints*(1.0-frac)))])\n",
    "\n",
    "counts, bin_edges = np.histogram(data, bins=Nbins, range=(xmin, xmax))\n",
    "unc_count = np.sqrt(counts)\n",
    "x = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(x, counts, yerr=unc_count, marker = 'o', ls='')\n",
    "ax.set_xlabel(f'x', fontsize=18)\n",
    "ax.set_ylabel(f'Frequency / {binwidth:.1f}', fontsize=18)\n",
    "ax.set_ylim(0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to fit the data with a bad model:\n",
    "\n",
    "We include the bin width (here 0.1) in the fit to ensure that the normalisations are (or could be) right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2exp_bad(x, N1, tau1, N2, tau2):\n",
    "    return binwidth * (N1*np.exp(-x/tau1) + N2*np.exp(-x/tau2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We actually start the fit with \"perfect\" initial values:\n",
    "mask = counts>0\n",
    "cfit = cost.LeastSquares(x[mask], counts[mask], unc_count[mask], func_2exp_bad)\n",
    "mfit = Minuit(cfit, N1=1000.0, tau1=2.0, N2=1000.0, tau2=10.0)\n",
    "mfit.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the covariance/correlation matrix:\n",
    "\n",
    "You want your fit to have the **least** correlations between the fitting parameters.<br>\n",
    "Try to check all the entries above, and see if you understand why the correlations are as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not mfit.fmin.is_valid) :                                   # Check if the fit converged\n",
    "    print(\"  WARNING: The ChiSquare fit DID NOT converge!!!\")\n",
    "\n",
    "ax.plot(x, func_2exp_bad(x, *mfit.values[:]), 'r', linewidth=2.0, label='Const + Gauss fit')\n",
    "\n",
    "# Adding fit results to plot:\n",
    "chi2 = mfit.fval                     # ChiSquare value\n",
    "Ndof = len(x[mask]) - mfit.nfit      # Number of (non-empty) bins\n",
    "Prob = stats.chi2.sf(chi2, Ndof)     # ChiSquare probability given Ndof\n",
    "\n",
    "fit_info = [f\"$\\\\chi^2$ / $N_\\\\mathrm{{dof}}$ = {chi2:.1f} / {Ndof}\", f\"Prob($\\\\chi^2$, $N_\\\\mathrm{{dof}}$) = {Prob:.3f}\",]\n",
    "for p, v, e in zip(mfit.parameters, mfit.values[:], mfit.errors[:]) :\n",
    "    Ndecimals = max(0,-np.int32(np.log10(e)-1-np.log10(2)))                                # Number of significant digits\n",
    "    fit_info.append(f\"{p} = ${v:{10}.{Ndecimals}{\"f\"}} \\\\pm {e:{10}.{Ndecimals}{\"f\"}}$\")\n",
    "\n",
    "ax.legend(title=\"\\n\".join(fit_info), fontsize=18, title_fontsize = 18, alignment = 'center');\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    " 1. Does the \"bad\" fit work well? Does it extract the parameters used to produce it?\n",
    "    Can you see what is missing? There are in fact several things, but one is simple to remedy.\n",
    "    Think and discuss...\n",
    "   \n",
    "---\n",
    "_5-10 minutes later_...\n",
    "\n",
    "---\n",
    "For once it is not the initial values! The reason is that the \"bad\" fitting function has two flaws:\n",
    "\n",
    "* It does not have a correct normalisation, thus making N1 and N2 correlated, as well as tau1 and tau2.\n",
    "* It does not have a single overall normalisation, thus making N1 and N2 even more correlated.\n",
    "\n",
    "This gives very high correlations between the parameters, as can be seen from the correlation matrix printed.\n",
    "\n",
    " 2. Both of these problems can be mitigated by rewriting the fitting function to include\n",
    "    the correct normalisation (i.e. dividing by the lifetime) and by putting only one\n",
    "    overall normalisation and then dividing the two lifetimes with a fraction (i.e. use\n",
    "    \"frac\" and \"(1.0-frac)\" as a parameter in front of each exponential term).\n",
    "    Try this (define a \"good\" function), and see if your fit improves. The way to see\n",
    "    this would in general be to try a lot of different data, but here we will simply see\n",
    "    that the correlations are smaller (especially for the overall normalisation).\n",
    "---\n",
    "_10-20 minutes later_...\n",
    "\n",
    "---  \n",
    "\n",
    "If you didn't manage to get this fit going, I've included a \"good\" fitting function below! (but try yourself first!)\n",
    "\n",
    " 3. The two lifetimes are naturally very correlated with each other (and the fraction),\n",
    "    when they are very alike. The only thing one can do about this is to fix one parameter.\n",
    "    Fixing a parameter is of course not desirable, but one can be forced to do it, if the fit does\n",
    "    not converge otherwise. Note that since the correlation is very high, it is not a great\n",
    "    loss of freedom in the fitting function. The correlation between tau1 and tau2 depends a lot\n",
    "    on how similar they are.<br>\n",
    "    Fix one parameter and see to what extend the fit becomes less correlated, the other parameters\n",
    "    more precisely determined, and the quality of the fit improves.\n",
    "\n",
    " 4. Finally, is it appropriate to fit the data with a ChiSquare? If not, then you should know what\n",
    "    to do instead. A simpler alternative might be to simply change the binning.\n",
    "\n",
    "---\n",
    "\n",
    "NOTE: A very similar and common example is fitting a \"Gaussian-like\" peak, which happens to have\n",
    "more than one width, for example if the data is obtained from two or more sources with\n",
    "different resolutions. Here, one may choose to let the two (or more) Gaussians have\n",
    "the same mean, but two different widths (the \"good\" and the \"bad\" measurements).\n",
    "Typically, the parameter to fix (if any) is the fraction, but never fix a parameter\n",
    "without first having tried to let it \"float\".\n",
    "    \n",
    "However, **no fit function is perfect** and the below fitting function is not an \"absolute\"\n",
    "improvement. The improvement lies only in some parts of the parameter space, and it is hard\n",
    "to know, when it has been improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GOOD fitting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2exp_good(x, N, frac, tau1, tau2):\n",
    "    return binwidth * N * (frac/tau1 * np.exp(-x/tau1) + (1.0-frac)/tau2 * np.exp(-x/tau2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "This exercise in \"fitting tricks\" should teach you that:\n",
    "1. __Good starting values is paramount!__ (Almost all fits fail with poor starting values).\n",
    "2. The form of the fitting function is also important.<br>\n",
    "   a. Ensure that the x-values do not represent some small range far from 0.<br>\n",
    "   b. Ensure that you give the fitting function enough freedom to fit the data.\n",
    "   c. Conversely, try to curb the number of parameters, if there are arguments for doing so (calibration peaks).\n",
    "   d. Make sure that you've normalised your fitting PDFs, to avoid correlations between normalisation and parameters.\n",
    "3. If a fit continues to fail, try simply to draw the function and starting values on top of the data. Often, they don't match well (general advice, not in this exercise)."
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
